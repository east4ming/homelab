loki-stack:
  loki:
    serviceMonitor:
      enabled: true
      prometheusRule:
        enabled: true
        rules:
          - alert: LokiProcessTooManyRestarts
            expr: changes(process_start_time_seconds{job=~"loki"}[15m]) > 2
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Loki process too many restarts (instance {{ $labels.instance }})
              description: "A loki process had too many restarts (target {{ $labels.instance }})\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: LokiRequestErrors
            expr: 100 * sum(rate(loki_request_duration_seconds_count{status_code=~"5.."}[1m])) by (namespace, job, route) / sum(rate(loki_request_duration_seconds_count[1m])) by (namespace, job, route) > 10
            for: 15m
            labels:
              severity: critical
            annotations:
              summary: Loki request errors (instance {{ $labels.instance }})
              description: "The {{ $labels.job }} and {{ $labels.route }} are experiencing errors\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: LokiRequestPanic
            expr: sum(increase(loki_panic_total[10m])) by (namespace, job) > 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: Loki request panic (instance {{ $labels.instance }})
              description: "The {{ $labels.job }} is experiencing {{ printf \"%.2f\" $value }}% increase of panics\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: LokiRequestLatency
            expr: (histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket{route!~"(?i).*tail.*"}[5m])) by (le)))  > 1
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: Loki request latency (instance {{ $labels.instance }})
              description: "The {{ $labels.job }} {{ $labels.route }} is experiencing {{ printf \"%.2f\" $value }}s 99th percentile latency\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
  promtail:
    config:
      snippets:
        extraScrapeConfigs: |
          # Add an additional scrape config for syslog
          - job_name: journal
            journal:
              path: /var/log/journal
              max_age: 12h
              labels:
                job: systemd-journal
            relabel_configs:
              - source_labels:
                  - __journal__hostname
                target_label: hostname

              # example label values: kubelet.service, containerd.service
              - source_labels:
                  - __journal__systemd_unit
                target_label: unit

              # example label values: debug, notice, info, warning, error
              - source_labels:
                  - __journal_priority_keyword
                target_label: level

    # Mount journal directory and machine-id file into promtail pods
    extraVolumes:
      - name: journal
        hostPath:
          path: /var/log/journal
      - name: machine-id
        hostPath:
          path: /etc/machine-id

    extraVolumeMounts:
      - name: journal
        mountPath: /var/log/journal
        readOnly: true
      - name: machine-id
        mountPath: /etc/machine-id
        readOnly: true
    serviceMonitor:
      enabled: true
      prometheusRule:
        enabled: true
        rules:
          - alert: PromtailRequestErrors
            expr: 100 * sum(rate(promtail_request_duration_seconds_count{status_code=~"5..|failed"}[1m])) by (namespace, job, route, instance) / sum(rate(promtail_request_duration_seconds_count[1m])) by (namespace, job, route, instance) > 10
            for: 5m
            labels:
              severity: critical
            annotations:
              description: "The {{ $labels.job }} {{ $labels.route }} is experiencing {{ printf \"%.2f\" $value }} errors.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
              summary: Promtail request errors (instance {{ $labels.instance }})
          - alert: PromtailRequestLatency
            expr: histogram_quantile(0.99, sum(rate(promtail_request_duration_seconds_bucket[5m])) by (le)) > 1
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: Promtail request latency (instance {{ $labels.instance }})
              description: "The {{ $labels.job }} {{ $labels.route }} is experiencing {{ printf \"%.2f\" $value }}s 99th percentile latency.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
